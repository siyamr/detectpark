{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approche DL pour Parkinson\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer les bibliotheques necessaires\n",
    "import os # interaction systeme\n",
    "import librosa # bibliotheque audio, spectro et extraction feature\n",
    "import numpy as np # tableau \n",
    "import pandas as pd # data frame\n",
    "import matplotlib.pyplot as plt # visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definir les chemins des dossiers contenant les fichiers audio\n",
    "path_to_hc = 'HC_AH/'\n",
    "path_to_pd = 'PD_AH/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les fichiers audio yo pour les patients sains\n",
    "y_hc = []\n",
    "sr_hc = []\n",
    "\n",
    "for file in os.listdir(path_to_hc):\n",
    "    if file.endswith('.wav'):\n",
    "        audio_path = os.path.join(path_to_hc, file)\n",
    "        y, sr = librosa.load(audio_path, sr=None)\n",
    "        y_hc.append(y)\n",
    "        sr_hc.append(sr)\n",
    "\n",
    "print('Nombre de patients sains:', len(y_hc))\n",
    "\n",
    "# Charger les fichiers audio pour les patients parkinsoniens\n",
    "y_pd = []\n",
    "sr_pd = []\n",
    "\n",
    "for file in os.listdir(path_to_pd):\n",
    "    if file.endswith('.wav'):\n",
    "        audio_path = os.path.join(path_to_pd, file)\n",
    "        y, sr = librosa.load(audio_path, sr=None)\n",
    "        y_pd.append(y)\n",
    "        sr_pd.append(sr)\n",
    "\n",
    "print('Nombre de patients parkinsoniens:', len(y_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On fixe la durée car pas la même durée pour tous les fichiers audio\n",
    "fixed_duration = 1.5\n",
    "\n",
    "# convertir un signal audio en spectrogramme de taille fixe\n",
    "def audio_to_fixed_spectrogram(y, sr, n_mels=128, fixed_length=None):\n",
    "    if fixed_length is not None:\n",
    "        y = librosa.util.fix_length(y, size=fixed_length) # Redimensionner le signal audio à une durée fixe\n",
    "    # Calculer le spectrogramme Mel\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    # Convertir en échelle logarithmique\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "    return S_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# longueur fixe en échantillons\n",
    "fixed_length = int(fixed_duration * sr_hc[1])\n",
    "print(fixed_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir tous les fichiers audio en spectrogrammes de la taille fixe définie\n",
    "X_hc = []\n",
    "for y, sr in zip(y_hc, sr_hc):\n",
    "    spectrogram = audio_to_fixed_spectrogram(y, sr, fixed_length=fixed_length)\n",
    "    X_hc.append(spectrogram)\n",
    "\n",
    "X_pd = []\n",
    "for y, sr in zip(y_pd, sr_pd):\n",
    "    spectrogram = audio_to_fixed_spectrogram(y, sr, fixed_length=fixed_length)\n",
    "    X_pd.append(spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir en tableaux NumPy\n",
    "X_hc = np.array(X_hc)\n",
    "X_pd = np.array(X_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verif les dimensions\n",
    "print(\"Shape de X_hc :\", X_hc.shape)\n",
    "print(\"Shape de X_pd :\", X_pd.shape)\n",
    "\n",
    "# Afficher un exemple de spectrogramme\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(X_hc[14], aspect='auto', origin='lower')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogramme Mel pour un patient sain')\n",
    "plt.ylabel('Echelle de Mel')\n",
    "plt.xlabel('Trame temporelle')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher un exemple de spectrogramme\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(X_pd[14], aspect='auto', origin='lower')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogramme Mel pour un patient parkinsonien')\n",
    "plt.ylabel('Echelle de Mel')\n",
    "plt.xlabel('Trame temporelle')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter une dimension pour les canaux pour pouvoir utiliser CNN\n",
    "X_hc = X_hc[..., np.newaxis]\n",
    "X_pd = X_pd[..., np.newaxis]\n",
    "\n",
    "# Verif les dimensions\n",
    "print(\"Shape de X_hc après ajout de canal :\", X_hc.shape)\n",
    "print(\"Shape de X_pd après ajout de canal :\", X_pd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# definir les labels de classes\n",
    "y_hc_labels = np.zeros(len(X_hc))  # 0 pour les patients sains\n",
    "y_pd_labels = np.ones(len(X_pd))   # 1 pour les patients parkinsoniens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combiner les données\n",
    "X = np.concatenate((X_hc, X_pd), axis=0)\n",
    "y = np.concatenate((y_hc_labels, y_pd_labels), axis=0)\n",
    "\n",
    "# Convertir les labels en catégories (one-hot encoding)\n",
    "y = to_categorical(y, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données en training set et test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Shape de X_train :\", X_train.shape)\n",
    "print(\"Shape de X_test :\", X_test.shape)\n",
    "print(\"Shape de y_train :\", y_train.shape)\n",
    "print(\"Shape de y_test :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle CNN classique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Architecture du CNN\n",
    "model = Sequential([\n",
    "    # Couche de convolution 2D\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=X_train.shape[1:]),\n",
    "    # Couche de pooling 2D pour réduire la dimensionnalité en gardant les informations importantes\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Deuxième couche de convolution et pooling\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Troisième couche de convolution et pooling\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Flatten pour convertir les matrices 2D des couches précédentes en vecteur 1D pour la couche fully connected\n",
    "    Flatten(),\n",
    "    \n",
    "    # Couche fully connected avec 128 neurones et fonction d'activation ReLU pour apprendre des fonctions non linéaires\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Dropout pour éviter le surapprentissage\n",
    "    \n",
    "    # Couche dense avec 2 neurones et fonction d'activation softmax pour la classification des 2 classes\n",
    "    Dense(2, activation='softmax')  # 2 classes (sain, parkinsonien)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiler le modèle\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher un résumé du modèle\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=70,  # Nombre d'époques\n",
    "    batch_size=32,  # Taille du batch\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les courbes d'apprentissage\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Accuracy (entraînement)')\n",
    "plt.plot(history.history['val_accuracy'], label='Accuracy (validation)')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Courbes d'apprentissage du CNN classique\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer le modèle sur les données de test\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy sur le test set :\", test_accuracy)\n",
    "\n",
    "# Faire des prédictions sur le test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Sain', 'Parkinsonien'], yticklabels=['Sain', 'Parkinsonien'])\n",
    "plt.xlabel('Prédit')\n",
    "plt.ylabel('Réel')\n",
    "plt.title('Matrice de confusion du CNN classique')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle CNN avec des couches plus profondes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Architecture du CNN profond\n",
    "model1 = Sequential([\n",
    "    # Première couche de convolution 2D\n",
    "    Conv2D(64, (3, 3), activation='relu', input_shape=X_train.shape[1:]),\n",
    "    # Couche de pooling 2D pour réduire la dimensionnalité\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Deuxième couche de convolution 2D\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    # Couche de pooling 2D\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Troisième couche de convolution 2D\n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    # Couche de pooling 2D\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Aplatir les données pour les couches fully connected\n",
    "    Flatten(),\n",
    "    \n",
    "    # Couche fully connected avec 256 neurones\n",
    "    Dense(256, activation='relu'),\n",
    "    # Dropout pour éviter le surapprentissage\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Couche de sortie avec 2 neurones (classification binaire)\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiler le modèle\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Afficher un résumé du modèle\n",
    "model1.summary()\n",
    "\n",
    "# Entraîner le modèle\n",
    "history1 = model1.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=60,  # Nombre d'époques\n",
    "    batch_size=32,  # Taille du batch\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "# Évaluer le modèle sur les données de test\n",
    "test_loss, test_accuracy = model1.evaluate(X_test, y_test)\n",
    "print(\"Accuracy sur le test set :\", test_accuracy)\n",
    "\n",
    "# Faire des prédictions sur le test set\n",
    "y_pred = model1.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tracer la précision \n",
    "plt.plot(history1.history['accuracy'], label='Accuracy (entraînement)')\n",
    "plt.plot(history1.history['val_accuracy'], label='Accuracy (validation)')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Courbes d'apprentissage du CNN profond\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Tracer la perte\n",
    "plt.plot(history1.history['loss'], label='Loss (entraînement)')\n",
    "plt.plot(history1.history['val_loss'], label='Loss (validation)')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Perte du CNN profond\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle CNN avec une régularisation L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Architecture du CNN avec Batch Normalization\n",
    "model2 = Sequential([\n",
    "    # Première couche de convolution 2D\n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001), input_shape=X_train.shape[1:]),\n",
    "    BatchNormalization(), # Normalisation des activations\n",
    "    MaxPooling2D((2, 2)), # Couche de pooling 2D\n",
    "    \n",
    "    # Deuxième couche de convolution 2D\n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(), # Normalisation des activations\n",
    "    MaxPooling2D((2, 2)), # Couche de pooling 2D\n",
    "    \n",
    "    # Troisième couche de convolution 2D\n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(), # Normalisation des activations\n",
    "    MaxPooling2D((2, 2)), # Couche de pooling 2D\n",
    "    \n",
    "    # Aplatir les données pour les couches fully connected\n",
    "    Flatten(),\n",
    "    \n",
    "    # Couche fully connected avec 128 neurones\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(), # Normalisation des activations\n",
    "    Dropout(0.5), # Dropout pour éviter le surapprentissage\n",
    "\n",
    "    # Couche de sortie avec 2 neurones (classification binaire)\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiler le modèle\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Afficher un résumé du modèle\n",
    "model2.summary()\n",
    "\n",
    "# Entraîner le modèle\n",
    "history2 = model2.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "# Tracer la précision\n",
    "plt.plot(history2.history['accuracy'], label='Accuracy (entraînement)')\n",
    "plt.plot(history2.history['val_accuracy'], label='Accuracy (validation)')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Courbes d'apprentissage du CNN avec régularisation L2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Tracer la perte \n",
    "plt.plot(history2.history['loss'], label='Loss (entraînement)')\n",
    "plt.plot(history2.history['val_loss'], label='Loss (validation)')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Perte du CNN profond avec Batch Normalization et régularisation L2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle CNN avec des couches séparables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SeparableConv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Architecture du CNN avec couches séparables\n",
    "model4 = Sequential([\n",
    "    # Première couche de convolution séparable\n",
    "    SeparableConv2D(32, (3, 3), activation='relu', input_shape=X_train.shape[1:]),\n",
    "    # Couche de pooling 2D pour réduire la dimensionnalité\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Deuxième couche de convolution séparable\n",
    "    SeparableConv2D(64, (3, 3), activation='relu'),\n",
    "    # Couche de pooling 2D\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Troisième couche de convolution séparable\n",
    "    SeparableConv2D(128, (3, 3), activation='relu'),\n",
    "    # Couche de pooling 2D\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Aplatir les données pour les couches fully connected\n",
    "    Flatten(),\n",
    "    \n",
    "    # Couche fully connected avec 128 neurones\n",
    "    Dense(128, activation='relu'),\n",
    "    # Dropout pour éviter le surapprentissage\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Couche de sortie avec 2 neurones (classification binaire)\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiler le modèle\n",
    "model4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Afficher un résumé du modèle\n",
    "model4.summary()\n",
    "\n",
    "# Entraîner le modèle\n",
    "history4 = model4.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=70,  # Nombre d'époques\n",
    "    batch_size=32,  # Taille du batch\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "# Évaluer le modèle sur les données de test\n",
    "test_loss, test_accuracy = model4.evaluate(X_test, y_test)\n",
    "print(\"Accuracy sur le test set :\", test_accuracy)\n",
    "\n",
    "# Faire des prédictions sur le test set\n",
    "y_pred = model4.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tracer la précision\n",
    "plt.plot(history4.history['accuracy'], label='Accuracy (entraînement)')\n",
    "plt.plot(history4.history['val_accuracy'], label='Accuracy (validation)')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Courbes d'apprentissage du CNN avec couches séparables\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Tracer la perte\n",
    "plt.plot(history4.history['loss'], label='Loss (entraînement)')\n",
    "plt.plot(history4.history['val_loss'], label='Loss (validation)')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Perte du CNN avec couches séparables\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle CNN allégé en paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SeparableConv2D, MaxPooling2D, GlobalAveragePooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Architecture du CNN léger avec couches séparables\n",
    "model_light = Sequential([\n",
    "    # Première couche de convolution séparable\n",
    "    SeparableConv2D(32, (3, 3), activation='relu', input_shape=X_train.shape[1:]),\n",
    "    # Couche de pooling 2D pour réduire la dimensionnalité\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Deuxième couche de convolution séparable\n",
    "    SeparableConv2D(64, (3, 3), activation='relu'),\n",
    "    # Couche de pooling 2D\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Troisième couche de convolution séparable\n",
    "    SeparableConv2D(128, (3, 3), activation='relu'),\n",
    "    # Global Average Pooling pour éviter la réduction excessive\n",
    "    GlobalAveragePooling2D(),\n",
    "    \n",
    "    # Couche fully connected avec 128 neurones\n",
    "    Dense(128, activation='relu'),\n",
    "    # Dropout pour éviter le surapprentissage\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Couche de sortie avec 2 neurones (classification binaire)\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiler le modèle\n",
    "model_light.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Afficher un résumé du modèle\n",
    "model_light.summary()\n",
    "\n",
    "# Entraîner le modèle\n",
    "history_light = model_light.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,  # Nombre d'époques\n",
    "    batch_size=32,  # Taille du batch\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "# Évaluer le modèle sur les données de test\n",
    "test_loss, test_accuracy = model_light.evaluate(X_test, y_test)\n",
    "print(\"Accuracy sur le test set :\", test_accuracy)\n",
    "\n",
    "# Faire des prédictions sur le test set\n",
    "y_pred = model_light.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tracer la précision\n",
    "plt.plot(history_light.history['accuracy'], label='Accuracy (entraînement)')\n",
    "plt.plot(history_light.history['val_accuracy'], label='Accuracy (validation)')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Courbes d'apprentissage du CNN léger\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Tracer la perte\n",
    "plt.plot(history_light.history['loss'], label='Loss (entraînement)')\n",
    "plt.plot(history_light.history['val_loss'], label='Loss (validation)')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Perte du CNN léger\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SeparableConv2D, MaxPooling2D, GlobalAveragePooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Architecture du CNN inspirée de VGG16 avec couches séparables\n",
    "model_vgg16 = Sequential([\n",
    "    # Bloc 1\n",
    "    SeparableConv2D(64, (3, 3), activation='relu', padding='same', input_shape=X_train.shape[1:]),\n",
    "    SeparableConv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Bloc 2\n",
    "    SeparableConv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    SeparableConv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Bloc 3\n",
    "    SeparableConv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    SeparableConv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    SeparableConv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Bloc 4\n",
    "    SeparableConv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "    SeparableConv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "    SeparableConv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "    GlobalAveragePooling2D(),  # Remplace MaxPooling2D pour éviter la réduction excessive\n",
    "    \n",
    "    # Couche fully connected\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),  # Dropout pour éviter le surapprentissage\n",
    "    \n",
    "    # Couche de sortie avec 2 neurones (classification binaire)\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiler le modèle\n",
    "model_vgg16.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Afficher un résumé du modèle\n",
    "model_vgg16.summary()\n",
    "\n",
    "# Entraîner le modèle\n",
    "history_vgg16 = model_vgg16.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,  # Nombre d'époques\n",
    "    batch_size=32,  # Taille du batch\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "# Évaluer le modèle sur les données de test\n",
    "test_loss, test_accuracy = model_vgg16.evaluate(X_test, y_test)\n",
    "print(\"Accuracy sur le test set :\", test_accuracy)\n",
    "\n",
    "# Faire des prédictions sur le test set\n",
    "y_pred = model_vgg16.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tracer la précision\n",
    "plt.plot(history_vgg16.history['accuracy'], label='Accuracy (entraînement)')\n",
    "plt.plot(history_vgg16.history['val_accuracy'], label='Accuracy (validation)')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Courbes d'apprentissage du CNN inspiré de VGG16\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Tracer la perte\n",
    "plt.plot(history_vgg16.history['loss'], label='Loss (entraînement)')\n",
    "plt.plot(history_vgg16.history['val_loss'], label='Loss (validation)')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Perte du CNN inspiré de VGG16\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SeparableConv2D, MaxPooling2D, GlobalAveragePooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Architecture du CNN inspirée de VGG19 avec couches séparables\n",
    "model_vgg19 = Sequential([\n",
    "    # Bloc 1\n",
    "    SeparableConv2D(64, (3, 3), activation='relu', padding='same', input_shape=X_train.shape[1:]),\n",
    "    SeparableConv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Bloc 2\n",
    "    SeparableConv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    SeparableConv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Bloc 3\n",
    "    SeparableConv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    SeparableConv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    SeparableConv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    SeparableConv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Bloc 4\n",
    "    SeparableConv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "    SeparableConv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "    SeparableConv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "    SeparableConv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "    GlobalAveragePooling2D(),  # Remplace MaxPooling2D pour éviter la réduction excessive\n",
    "    \n",
    "    # Couche fully connected\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),  # Dropout pour éviter le surapprentissage\n",
    "    \n",
    "    # Couche de sortie avec 2 neurones (classification binaire)\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiler le modèle\n",
    "model_vgg19.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Afficher un résumé du modèle\n",
    "model_vgg19.summary()\n",
    "\n",
    "# Entraîner le modèle\n",
    "history_vgg19 = model_vgg19.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,  # Nombre d'époques\n",
    "    batch_size=32,  # Taille du batch\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "# Évaluer le modèle sur les données de test\n",
    "test_loss, test_accuracy = model_vgg19.evaluate(X_test, y_test)\n",
    "print(\"Accuracy sur le test set :\", test_accuracy)\n",
    "\n",
    "# Faire des prédictions sur le test set\n",
    "y_pred = model_vgg19.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tracer la précision\n",
    "plt.plot(history_vgg19.history['accuracy'], label='Accuracy (entraînement)')\n",
    "plt.plot(history_vgg19.history['val_accuracy'], label='Accuracy (validation)')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Courbes d'apprentissage du CNN inspiré de VGG19\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Tracer la perte\n",
    "plt.plot(history_vgg19.history['loss'], label='Loss (entraînement)')\n",
    "plt.plot(history_vgg19.history['val_loss'], label='Loss (validation)')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Perte du CNN inspiré de VGG19\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle inspiré de ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Add, Activation, BatchNormalization, SeparableConv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Fonction pour créer un bloc résiduel\n",
    "def residual_block(x, filters, kernel_size=(3, 3), strides=(1, 1)):\n",
    "    # Branche principale\n",
    "    # Convolution 2D séparable\n",
    "    y = SeparableConv2D(filters, kernel_size, strides=strides, padding='same')(x)\n",
    "    y = BatchNormalization()(y) # Normalisation des activations\n",
    "    y = Activation('relu')(y) # Fonction d'activation ReLU\n",
    "    \n",
    "    y = SeparableConv2D(filters, kernel_size, padding='same')(y) # Convolution 2D séparable\n",
    "    y = BatchNormalization()(y) # Normalisation des activations\n",
    "    \n",
    "    # Connexion skip\n",
    "    if strides != (1, 1) or x.shape[-1] != filters:\n",
    "        x = SeparableConv2D(filters, (1, 1), strides=strides, padding='same')(x) # Convolution 2D séparable\n",
    "        x = BatchNormalization()(x) # Normalisation des activations\n",
    "    \n",
    "    # Ajouter la branche principale et la connexion skip\n",
    "    out = Add()([x, y]) # Ajout des deux branches\n",
    "    out = Activation('relu')(out) # Fonction d'activation ReLU\n",
    "    return out\n",
    "\n",
    "# Modèle ResNet-like\n",
    "inputs = Input(shape=X_train.shape[1:])  # Entrée du modèle\n",
    "x = SeparableConv2D(64, (7, 7), strides=(2, 2), padding='same')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "# Blocs résiduels\n",
    "x = residual_block(x, filters=64)\n",
    "x = residual_block(x, filters=64)\n",
    "x = residual_block(x, filters=128, strides=(2, 2))\n",
    "x = residual_block(x, filters=128)\n",
    "x = residual_block(x, filters=256, strides=(2, 2))\n",
    "x = residual_block(x, filters=256)\n",
    "\n",
    "# Global Average Pooling pour réduire la dimensionnalité\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Couche fully connected\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)  # Dropout pour éviter le surapprentissage\n",
    "\n",
    "# Couche de sortie\n",
    "outputs = Dense(2, activation='softmax')(x)  # Classification binaire\n",
    "\n",
    "# Créer le modèle\n",
    "model_resnet = Model(inputs, outputs)\n",
    "\n",
    "# Compiler le modèle\n",
    "model_resnet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Afficher un résumé du modèle\n",
    "model_resnet.summary()\n",
    "\n",
    "# Entraîner le modèle\n",
    "history_resnet = model_resnet.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,  # Nombre d'époques\n",
    "    batch_size=32,  # Taille du batch\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "# Évaluer le modèle sur les données de test\n",
    "test_loss, test_accuracy = model_resnet.evaluate(X_test, y_test)\n",
    "print(\"Accuracy sur le test set :\", test_accuracy)\n",
    "\n",
    "# Faire des prédictions sur le test set\n",
    "y_pred = model_resnet.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tracer la précision\n",
    "plt.plot(history_resnet.history['accuracy'], label='Accuracy (entraînement)')\n",
    "plt.plot(history_resnet.history['val_accuracy'], label='Accuracy (validation)')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Courbes d'apprentissage du CNN ResNet-like\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Tracer la perte\n",
    "plt.plot(history_resnet.history['loss'], label='Loss (entraînement)')\n",
    "plt.plot(history_resnet.history['val_loss'], label='Loss (validation)')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Perte du CNN ResNet-like\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dupliquer le canal unique pour créer 3 canaux\n",
    "X_train_rgb = np.repeat(X_train, 3, axis=-1)\n",
    "X_test_rgb = np.repeat(X_test, 3, axis=-1)\n",
    "\n",
    "# Vérifier la nouvelle forme\n",
    "print(\"Shape de X_train_rgb :\", X_train_rgb.shape)\n",
    "print(\"Shape de X_test_rgb :\", X_test_rgb.shape)\n",
    "\n",
    "# Redimensionner les spectrogrammes à 128x128 pixels\n",
    "X_train_resized = tf.image.resize(X_train_rgb, [128, 128])\n",
    "X_test_resized = tf.image.resize(X_test_rgb, [128, 128])\n",
    "\n",
    "# Vérifier la nouvelle forme\n",
    "print(\"Shape de X_train_resized :\", X_train_resized.shape)\n",
    "print(\"Shape de X_test_resized :\", X_test_resized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Charger InceptionV3 avec la nouvelle taille d'entrée\n",
    "model5 = InceptionV3(\n",
    "    include_top=False,  # Ne pas inclure la couche fully connected finale\n",
    "    input_shape=(128, 128, 3),  # Nouvelle forme avec 3 canaux\n",
    "    pooling='avg',  # Global Average Pooling pour réduire la dimensionnalité\n",
    "    weights='imagenet'  # Utiliser les poids préentraînés sur ImageNet\n",
    ")\n",
    "\n",
    "# Geler les couches du modèle préentraîné\n",
    "model5.trainable = False\n",
    "\n",
    "# Ajouter une couche fully connected pour la classification\n",
    "x = model5.output\n",
    "x = Dense(128, activation='relu')(x)  # Couche fully connected\n",
    "x = Dropout(0.5)(x)  # Dropout pour éviter le surapprentissage\n",
    "predictions = Dense(2, activation='softmax')(x)  # Couche de sortie\n",
    "\n",
    "# Créer le modèle final\n",
    "model5_final = Model(inputs=model5.input, outputs=predictions)\n",
    "\n",
    "# Compiler le modèle\n",
    "model5_final.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Afficher un résumé du modèle\n",
    "model5_final.summary()\n",
    "\n",
    "# Entraîner le modèle\n",
    "history5 = model5_final.fit(\n",
    "    X_train_resized, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_resized, y_test)\n",
    ")\n",
    "\n",
    "\n",
    "# Visualiser les courbes d'apprentissage\n",
    "plt.plot(history5.history['accuracy'], label='Accuracy (entraînement)')\n",
    "plt.plot(history5.history['val_accuracy'], label='Accuracy (validation)')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Courbes d'apprentissage d'InceptionV3\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer le modèle sur les données de test\n",
    "test_loss, test_accuracy = model5_final.evaluate(X_test_resized, y_test)\n",
    "print(\"Accuracy sur le test set :\", test_accuracy)\n",
    "\n",
    "# Faire des prédictions sur le test set\n",
    "y_pred = model5_final.predict(X_test_resized)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "InceptionV3 avec une taille de spectrogramme 600x600pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dupliquer le canal unique pour créer 3 canaux\n",
    "X_train_rgb = np.repeat(X_train, 3, axis=-1)\n",
    "X_test_rgb = np.repeat(X_test, 3, axis=-1)\n",
    "\n",
    "# Vérifier la nouvelle forme\n",
    "print(\"Shape de X_train_rgb :\", X_train_rgb.shape)\n",
    "print(\"Shape de X_test_rgb :\", X_test_rgb.shape)\n",
    "\n",
    "# Redimensionner les spectrogrammes à 128x128 pixels\n",
    "X_train_resized = tf.image.resize(X_train_rgb, [600, 600])\n",
    "X_test_resized = tf.image.resize(X_test_rgb, [600, 600])\n",
    "\n",
    "# Vérifier la nouvelle forme\n",
    "print(\"Shape de X_train_resized :\", X_train_resized.shape)\n",
    "print(\"Shape de X_test_resized :\", X_test_resized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger InceptionV3 avec la nouvelle taille d'entrée\n",
    "base_model = InceptionV3(\n",
    "    include_top=False,  # Ne pas inclure la couche fully connected finale\n",
    "    input_shape=(600, 600, 3),  # Nouvelle forme avec 3 canaux\n",
    "    weights='imagenet'  # Utiliser les poids préentraînés sur ImageNet\n",
    ")\n",
    "\n",
    "# Geler les couches du modèle préentraîné\n",
    "base_model.trainable = False\n",
    "\n",
    "# Ajouter une couche fully connected pour la classification\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)  # Aplatir les dimensions spatiales\n",
    "x = Dense(128, activation='relu')(x)  # Couche fully connected\n",
    "x = Dropout(0.5)(x)  # Dropout pour éviter le surapprentissage\n",
    "predictions = Dense(2, activation='softmax')(x)  # Couche de sortie\n",
    "\n",
    "# Créer le modèle final\n",
    "model5_final = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compiler le modèle\n",
    "model5_final.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Afficher un résumé du modèle\n",
    "model5_final.summary()\n",
    "\n",
    "# Entraîner le modèle\n",
    "history5 = model5_final.fit(\n",
    "    X_train_resized, y_train,\n",
    "    epochs=10,  # Nombre d'époques\n",
    "    batch_size=4,  # Taille du batch\n",
    "    validation_data=(X_test_resized, y_test)\n",
    ")\n",
    "\n",
    "# Visualiser les courbes d'apprentissage\n",
    "plt.plot(history5.history['accuracy'], label='Accuracy (entraînement)')\n",
    "plt.plot(history5.history['val_accuracy'], label='Accuracy (validation)')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Courbes d'apprentissage d'InceptionV3\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer le modèle sur les données de test\n",
    "test_loss, test_accuracy = model5_final.evaluate(X_test_resized, y_test)\n",
    "print(\"Accuracy sur le test set :\", test_accuracy)\n",
    "\n",
    "# Faire des prédictions sur le test set\n",
    "y_pred = model5_final.predict(X_test_resized)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
